{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648eda89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6a82b2-80bd-42f0-acfc-092db776463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gbtoolbox.bounds as bounds\n",
    "import gbtoolbox.misc as mt\n",
    "import gbtoolbox.dft as dft\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "print(torch.__version__)\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e93f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 80\n",
    "mult = 2*2\n",
    "span = 2\n",
    "x = (np.random.rand(mult*N).reshape(-1,2)*2-1)*span/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aeacdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132da133",
   "metadata": {},
   "source": [
    "Note that in the theory, and in the toolbox, the input space is $[-1,1]^d$. So if your space is $[0,2\\pi]^d$ for example, the input needs scaled properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429be2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.sin(2.1*1*np.pi*x[:,0])*np.sin(3.4*1*np.pi*x[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9958cfa5-0f18-49d0-8f51-33debc796789",
   "metadata": {},
   "source": [
    "We look at a simple function that can be understood analytically to highlight features and use of this toolbox. The function here is $y = \\sin(2.1 \\pi x_0)\\sin(3.4 \\pi x_1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce20e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, _ = mt.gen_stacked_equispaced_nd_grid(N,np.array([[-Bt/2.0,Bt/2.0] for Bt in np.array([N/span,N/span])]))\n",
    "V = span*span\n",
    "spans = np.tile([-span/2.0,span/2.0],(2,1))\n",
    "print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbca40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yf = (V/x.shape[0])*dft.nu_dft_cuda(x,y,f,256,128)/(np.sqrt(2*np.pi)**2)\n",
    "print(yf.shape)\n",
    "plt.scatter(f[:,0],np.abs(yf))\n",
    "plt.scatter(f[:,1],np.abs(yf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef38fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.abs(yf).reshape(N,N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97805eac",
   "metadata": {},
   "source": [
    "This gives an initial look, we see some noise induced sampling that is down at 0.03. We also see some extended tails going up/down. I would recommend looking at several plots when making cuts, in this one there are no peaks between the ones expected from the analytical calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c487e216",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, _ = mt.gen_stacked_equispaced_nd_grid(N,np.array([[-Bt/4.0,Bt/4.0] for Bt in np.array([N/span,N/span])]))\n",
    "V = span*span\n",
    "spans = np.tile([-span/2.0,span/2.0],(2,1))\n",
    "print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3433f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "yf = (V/x.shape[0])*dft.nu_dft_cuda(x,y,f,256,128)/(np.sqrt(2*np.pi)**2)\n",
    "print(yf.shape)\n",
    "plt.scatter(f[:,0],np.abs(yf))\n",
    "plt.scatter(f[:,1],np.abs(yf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45dc45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.abs(yf).reshape(N,N))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bae0b0",
   "metadata": {},
   "source": [
    "Here instead of an extended line, we see some side lobes, which is suggestive. We also see a lot of gap in the center, but not on one axis. This should have been seen in the previous plot if it was real. So already we are inclined to cut above maybe 0.05 if we use this approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ca0d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, _ = mt.gen_stacked_equispaced_nd_grid(N,np.array([[-Bt,Bt] for Bt in np.array([N/span,N/span])]))\n",
    "V = span*span\n",
    "spans = np.tile([-span/2.0,span/2.0],(2,1))\n",
    "print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768fadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "yf = (V/x.shape[0])*dft.nu_dft_cuda(x,y,f,256,128)/(np.sqrt(2*np.pi)**2)\n",
    "print(yf.shape)\n",
    "plt.scatter(f[:,0],np.abs(yf))\n",
    "plt.scatter(f[:,1],np.abs(yf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d77e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.abs(yf).reshape(N,N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3154e162",
   "metadata": {},
   "source": [
    "Remember that $w=2\\pi f$. Note that we can see two factors here, one with the extension (side lobes in other resolutions) and the other is the random sampling error induced by the low number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4032e50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, _ = mt.gen_stacked_equispaced_nd_grid(N*2,np.array([[-Bt,Bt] for Bt in np.array([4,4])]))\n",
    "V = span*span\n",
    "spans = np.tile([-1,1],(2,1))\n",
    "print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58125f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "yf = (V/x.shape[0])*dft.nu_dft_cuda(x,y,f,256,128)/(np.sqrt(2*np.pi)**2)\n",
    "print(yf.shape)\n",
    "plt.scatter(f[:,0],np.abs(yf))\n",
    "plt.scatter(f[:,1],np.abs(yf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad0fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.abs(yf).reshape(N*2,N*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14afad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = dft.threshold_cmask(yf,0.130)\n",
    "plt.imshow(np.abs(yf*mask.astype(int)).reshape(N*2,N*2),extent=[-4,4,-4,4])\n",
    "plt.title('Approximate $\\int\\int\\sin(2.1\\pi x)\\sin(3.4\\pi y)e^{i x w + i k y} dx dy$')\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f72a8c",
   "metadata": {},
   "source": [
    "In these plots we cut for just the 4 peaks. We can clearly cut the other peaks because we see them move as we adjust the resolution in $f$. How tightly to cut the 4 peaks is something that might not be known in general, although we can try periodization to see how wide the real peak is. Here I am making a tight cut, but it could be tighter.\n",
    "\n",
    "If we increased the statistics, the peaks would become even in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1002215",
   "metadata": {},
   "outputs": [],
   "source": [
    "sne = bounds.est_spec_norm(f*2*np.pi,yf,None,mask)\n",
    "print(sne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1643f352",
   "metadata": {},
   "source": [
    "This is much larger than the analytic calculation, but this is to be expected due the nature of the problem and the nature of the non-equispaced to equispaced FT. Often times we end up with smaller Spectral Norms. This goes into the calculation of the a priori bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bbc6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnd = bounds.apriori_bound(sne,45000,N*mult/2,2,0.95)\n",
    "print(abnd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e03dfe",
   "metadata": {},
   "source": [
    "The bound is high, but then the Barron norm/Spectral norm is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ce364",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(np.array([1e3,5e3,1e4,5e4,1e5,5e5,1e6,1e7])),[bounds.apriori_bound(sne,MM,N*mult,2,0.95) for MM in np.array([1e3,5e3,1e4,5e4,1e5,5e5,1e6,1e7])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ba3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array([1e3,5e3,1e4,5e4]),[bounds.apriori_bound(sne,MM,N*mult,2,0.95) for MM in np.array([1e3,5e3,1e4,5e4])])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3aaa55",
   "metadata": {},
   "source": [
    "We see the expected limiting behavior as a function of number of nodes. Do we see the right behavior if we keep the Spectral norm constant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6566149",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(np.array([1e3,5e3,1e4,5e4,1e5,5e5,1e6,1e7])),[bounds.apriori_bound(sne,1e5,NN,2,0.95) for NN in np.array([1e3,5e3,1e4,5e4,1e5,5e5,1e6,1e7])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5b8f7",
   "metadata": {},
   "source": [
    "Now, while the Spectral norm/Barron norm is large, we can still use this as part of our initialization. We can then apply the mask to the pdf from the Fourier analytic Barron theory.\n",
    "\n",
    "Note that to actually get a useful bound, we need to increase statistics (NN), which would also allow us to find a more accurate Barron norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbb0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf=bounds.E_pdf((mask.astype(int)*yf),2*np.pi*((f).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "MM=45000\n",
    "zv, tv, wv, sv = pdf.gen_ztw_c(MM)\n",
    "wh, wn = bounds.nn_wnorm(wv)\n",
    "print(wh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8793265f",
   "metadata": {},
   "source": [
    "For the probability density function (pdf), we also use $2\\pi f$. Also, if you can initialize well, then the larger the number of nodes the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a51fa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2d,x2d,y2d,i2d=plt.hist2d(wh.T[:,0],wh.T[:,1],bins=100)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a969a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2d,x2d,y2d,i2d=plt.hist2d(wh.T[:,0],wh.T[:,1],bins=100,weights=sv)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9aadc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(h2d[:40,:40]))\n",
    "print(np.sum(h2d[:40,60:]))\n",
    "print(np.sum(h2d[60:,:40]))\n",
    "print(np.sum(h2d[60:,60:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1288fe",
   "metadata": {},
   "source": [
    "Here we know the answer, so we know that each 'peak' should be equal in magnitude but two negative and two positive. We see a smallish assymetry here, and it is this assymetry, from the outer weight sign, that drives the linear gradient that can be very strong in the approximation. The sign will be more accurate the more accurate the approximate FT is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(zv)\n",
    "plt.hist(sv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09171aa",
   "metadata": {},
   "source": [
    "One possible solution is to provide both -1 and 1 to the network, with some small 'noise' difference in magnitude, and then let training the neural network move the wrong sign to 0. We don't expect the argument of the approximate Fourier transform to be as well determined as the magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa183ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tv*zv,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79f122c",
   "metadata": {},
   "source": [
    "This  behaves well here. Since we have only a single value for $\\|w\\|_1$, analytically, we would expect behavior that is approximately $|\\cos(\\|w\\|_1 t)|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c35743",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a7a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x = torch.Tensor(x) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55def5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.backends.cudnn.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6572fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ShallowRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ShallowRegressor,self).__init__()\n",
    "        self.fc1 = nn.Linear(2,MM)\n",
    "        self.fc2 = nn.Linear(MM,1)     \n",
    "        self.fc1.weight = torch.nn.Parameter(torch.Tensor(wh.T))\n",
    "        self.fc1.bias = torch.nn.Parameter(torch.Tensor(tv*zv))      \n",
    "        self.fc2.bias = torch.nn.Parameter(torch.Tensor(np.array([np.mean(y)])))\n",
    "        self.fc2.weight = torch.nn.Parameter(torch.Tensor((sne)*(sv).reshape(-1,1).T/MM))\n",
    "      \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298302c9",
   "metadata": {},
   "source": [
    "We can improve in practice, note that since our value for the Barron norm is significantly off, that the overall scale (from the outer weights) is going to be off. We provide a correction for that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b3094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ShallowRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa3d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470ce9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f061ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor_x.shape)\n",
    "ABS = 60\n",
    "XX = np.linspace(-1,1,ABS,endpoint=False)\n",
    "XXg = mt.grid_to_stack(np.meshgrid(XX,XX))\n",
    "print(XXg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cccf673",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "YYg = model(torch.Tensor(XXg).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f5b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((np.sin(2.1*1*np.pi*XXg[:,0])*np.sin(3.4*1*np.pi*XXg[:,1])).reshape(ABS,ABS),extent=[-1, 1, -1, 1])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7193155",
   "metadata": {},
   "source": [
    "We see a nice simple pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e62a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(YYg.shape)\n",
    "plt.imshow(YYg.detach().cpu().numpy().reshape(ABS,ABS),extent=[-1,1, -1, 1])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afd4dc1",
   "metadata": {},
   "source": [
    "We see the same patern, but probably with a different scale (maybe even more than 10x too high) and with some roughly linear gradient. This is because of our lower approximate Fourier transform. If the 4 peaks are a bit unbalanced due to the number of samples, we will get a gradient. If we have a too large Spectral norm, we will be too high. If we don't cut agressively enough and include a lot more of the region around the peak, we will have a gradient. This all requires some work. \n",
    "\n",
    "In general we want to do as good of job as we can, and then possibly do additional corrections with training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd7bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(YYg.shape)\n",
    "maxYY=np.max(YYg.detach().cpu().numpy())\n",
    "maxXX=np.max((np.sin(2.1*1*np.pi*XXg[:,0])*np.sin(3.4*1*np.pi*XXg[:,1])))\n",
    "factorYY=np.max(YYg.detach().cpu().numpy())-np.min(YYg.detach().cpu().numpy())\n",
    "factorXX=np.max((np.sin(2.1*1*np.pi*XXg[:,0])*np.sin(3.4*1*np.pi*XXg[:,1])))-np.min((np.sin(2.1*1*np.pi*XXg[:,0])*np.sin(3.4*1*np.pi*XXg[:,1])))\n",
    "print(factorYY)\n",
    "print(factorXX)\n",
    "print(maxYY/factorYY*factorXX+maxXX)\n",
    "plt.imshow(YYg.detach().cpu().numpy().reshape(ABS,ABS)/factorYY*factorXX-maxYY/factorYY*factorXX+maxXX,extent=[-1,1, -1, 1])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dff4be",
   "metadata": {},
   "source": [
    "Here we use the truth to scale, but in general practice it may be a good idea to use the predictions of the training set from the initial initialization to rescale the outer weights. So the weights are initialized with a second process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a48c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = (np.random.rand(mult*N*1).reshape(-1,2)*2-1)*span/2.0\n",
    "yt = np.sin(2.1*1*np.pi*xt[:,0])*np.sin(3.4*1*np.pi*xt[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1a5e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xt[:,0], xt[:,1], c=(np.sin(2.1*1*np.pi*xt[:,0])*np.sin(3.4*1*np.pi*xt[:,1])))\n",
    "plt.colorbar()\n",
    "plt.title('True Test Set for $\\sin(2.1\\pi x)\\sin(3.4\\pi y)$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xt[:,0], xt[:,1], c=model(torch.Tensor(xt).to(device)).detach().cpu().numpy())\n",
    "plt.colorbar()\n",
    "plt.title('Predicted Test Set for $\\sin(2.1\\pi x)\\sin(3.4\\pi y)$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d93f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = model(torch.Tensor(xt).to(device)).detach().cpu().numpy()/factorYY*factorXX-maxYY/factorYY*factorXX+maxXX\n",
    "plt.scatter(xt[:,0], xt[:,1], c=scaled)\n",
    "plt.colorbar()\n",
    "plt.title('Scaled Predicted Test Set for $\\sin(2.1\\pi x)\\sin(3.4\\pi y)$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d045940",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.square(np.subtract((np.sin(2.1*1*np.pi*xt[:,0])*np.sin(3.4*1*np.pi*xt[:,1])).reshape(-1,1), model(torch.Tensor(xt).to(device)).detach().cpu().numpy()/factorYY*factorXX-maxYY/factorYY*factorXX+maxXX)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deff757",
   "metadata": {},
   "source": [
    "While we do get many of the correct overall features, even from the few data points, we do get an overall gradient which could be corrected for with additional training. The shape is a little different, but this will improve as the approximate Fourier transform is improved (either by a better threshold or by having more sample data).\n",
    "\n",
    "For a shallow neural network, we found a close prediction fo the truth based off of a fairly small data sample.\n",
    "\n",
    "The MSE is good but not incredible, but improves as the number of samples provided to the approximate Fourier transform increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c72851",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:,0], x[:,1], c=y)\n",
    "plt.colorbar()\n",
    "plt.title('True Train Set for $\\sin(2.1\\pi x)\\sin(3.4\\pi y)$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf9c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowRegressor2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ShallowRegressor2,self).__init__()\n",
    "        self.fc1 = nn.Linear(2,MM)\n",
    "        self.fc2 = nn.Linear(MM,1)     \n",
    "        #self.fc1.weight = torch.nn.Parameter(torch.Tensor(wh.T))\n",
    "        #self.fc1.bias = torch.nn.Parameter(torch.Tensor(tv*zv))      \n",
    "        #self.fc2.bias = torch.nn.Parameter(torch.Tensor(np.array([np.mean(y)])))\n",
    "        #self.fc2.weight = torch.nn.Parameter(torch.Tensor((sne)*(sv).reshape(-1,1).T/MM))\n",
    "      \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e1309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ShallowRegressor2()\n",
    "model2 = model2.to(device)\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095af3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xt[:,0], xt[:,1], c=model2(torch.Tensor(xt).to(device)).detach().cpu().numpy())\n",
    "plt.colorbar()\n",
    "plt.title('Predicted Test Set for Default Initialization')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997255a",
   "metadata": {},
   "source": [
    "If we look at the standard initialization we find that we get something that doesn't look anything like the target function. Additionally, the Mean Square Error is signficantly higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656017b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.square(np.subtract((np.sin(2.1*1*np.pi*xt[:,0])*np.sin(3.4*1*np.pi*xt[:,1])).reshape(-1,1), model2(torch.Tensor(xt).to(device)).detach().cpu().numpy()/factorYY*factorXX-maxYY/factorYY*factorXX+maxXX)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1828dc88",
   "metadata": {},
   "source": [
    "A good initialization allows a neural network to produce a good approximation both faster and a better quality approximation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725be4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
